{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! CUDA_VISIBLE_DEVICES='1' python /workspace/TensorRT/demo/BERT/python/bert_builder.py -m /workspace/TensorRT/demo/BERT/eval_ckpts/model.ckpt-315171 -o tmp.engine -b 32 -s 8 -c /workspace/TensorRT/demo/BERT/eval_ckpts -t cola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls /workspace/TensorRT/demo/BERT/build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Let's convert the paragraph and the question to BERT input with the help of the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import data_processing as dp\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/workspace/TensorRT/demo/BERT/eval_ckpts/data.txt', 'r', encoding='utf-8') as fin:\n",
    "    tmp = fin.readlines()\n",
    "data = []\n",
    "for i in range(1, len(tmp)):\n",
    "    label, sent = tmp[i].split('\\t')\n",
    "    data.append([label, sent])\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(vocab_file=\"/workspace/models/fine-tuned/bert_tf_v2_base_fp16_128_v2/vocab.txt\", do_lower_case=True)\n",
    "\n",
    "# The maximum number of tokens for the question. Questions longer than this will be truncated to this length.\n",
    "max_query_length = 8\n",
    "\n",
    "# When splitting up a long document into chunks, how much stride to take between chunks.\n",
    "doc_stride = 8\n",
    "\n",
    "# The maximum total input sequence length after WordPiece tokenization. \n",
    "# Sequences longer than this will be truncated, and sequences shorter \n",
    "max_seq_length = 8\n",
    "\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CDLL '/workspace/TensorRT/demo/BERT/build/libbert_plugins.so', handle 47fa0c0 at 0x7f5b80643be0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ctypes\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "ctypes.CDLL(\"libnvinfer_plugin.so\", mode=ctypes.RTLD_GLOBAL)\n",
    "ctypes.CDLL(\"/workspace/TensorRT/demo/BERT/build/libcommon.so\", mode=ctypes.RTLD_GLOBAL)\n",
    "ctypes.CDLL(\"/workspace/TensorRT/demo/BERT/build/libbert_plugins.so\", mode=ctypes.RTLD_GLOBAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "Running Inference...\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "-----------------------------\n",
      "Running Inference in 413.291 Batches/Sec\n",
      "Time using for one batch is 2.419603 ms\n",
      "Average time using for one inference is 2.419603 ms\n",
      "Time using for one batch cold start is 7.354 s\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "import time\n",
    "cold_start = time.time()\n",
    "# Load the BERT-Large Engine\n",
    "with open(\"bert_shopee_8_batch32.engine\", \"rb\") as f, \\\n",
    "    trt.Runtime(TRT_LOGGER) as runtime, \\\n",
    "    runtime.deserialize_cuda_engine(f.read()) as engine, \\\n",
    "    engine.create_execution_context() as context:\n",
    "    \n",
    "    # print(engine.max_batch_size, engine.num_layers, engine.num_bindings, engine.num_optimization_profiles)\n",
    "    # change the optimization profile to fit in input data\n",
    "    \n",
    "    print(context.active_optimization_profile)\n",
    "#     context.active_optimization_profile = 1\n",
    "#     for i in range(engine.num_optimization_profiles):\n",
    "#         if i == context.active_optimization_profile:\n",
    "#             for binding in range(3):\n",
    "#                 print(i, engine.get_profile_shape(profile_index=i, binding=binding))\n",
    "\n",
    "    # We always use batch size 1.( From nvidia)\n",
    "    # able to use larger batch size\n",
    "    \n",
    "    input_shape = (batch_size, max_seq_length)\n",
    "    input_nbytes = trt.volume(input_shape) * trt.int32.itemsize\n",
    "    \n",
    "    # Allocate device memory for inputs.\n",
    "    d_inputs = [cuda.mem_alloc(input_nbytes) for binding in range(3)]\n",
    "    # Create a stream in which to copy inputs/outputs and run inference.\n",
    "    stream = cuda.Stream()\n",
    "\n",
    "    # Specify input shapes. These must be within the min/max bounds of the active profile (0th profile in this case)\n",
    "    # Note that input shapes can be specified on a per-inference basis, but in this case, we only have a single shape.\n",
    "    for binding in range(3):\n",
    "        context.set_binding_shape(binding, input_shape)\n",
    "    assert context.all_binding_shapes_specified\n",
    "    # Allocate output buffer by querying the size from the context. This may be different for different input shapes.\n",
    "    h_output = cuda.pagelocked_empty(tuple(context.get_binding_shape(3)), dtype=np.float32)\n",
    "    d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "\n",
    "    print(\"\\nRunning Inference...\")\n",
    "    ttl_time = 0\n",
    "    correct = 0\n",
    "\n",
    "    for step in range(len(data)//batch_size):\n",
    "        eval_start_time = time.time()            \n",
    "        input_ids = np.random.randn(batch_size, 8)\n",
    "        segment_ids = np.random.randn(batch_size, 8)\n",
    "        input_mask = np.random.randn(batch_size, 8)\n",
    "        label = np.zeros(batch_size)\n",
    "        for i in range(batch_size):\n",
    "            short_paragraph_text = data[step * batch_size + i][1]\n",
    "            doc_tokens = dp.convert_doc_tokens(short_paragraph_text)\n",
    "            try:\n",
    "                features = dp.convert_examples_to_features(\n",
    "                    doc_tokens, '', tokenizer, max_seq_length, doc_stride, max_query_length\n",
    "                )\n",
    "                input_ids[i] = features['input_ids']\n",
    "                segment_ids[i] = features['segment_ids']\n",
    "                input_mask[i] = features['input_mask']\n",
    "                label[i] = int(data[step * batch_size + i][0])\n",
    "            except:\n",
    "                print(doc_tokens)\n",
    "                i -= 1\n",
    "        buffer_time = time.time()\n",
    "        \n",
    "        # asynchronous execution\n",
    "        # Copy inputs(np arrays) into cuda memory\n",
    "        cuda.memcpy_htod_async(d_inputs[0], input_ids.astype(np.int32), stream)\n",
    "        cuda.memcpy_htod_async(d_inputs[1], segment_ids.astype(np.int32), stream)\n",
    "        cuda.memcpy_htod_async(d_inputs[2], input_mask.astype(np.int32), stream)\n",
    "        # Run inference, inference result is stored in cuda memory\n",
    "        context.execute_async_v2(bindings=[int(d_inp) for d_inp in d_inputs] + [int(d_output)], stream_handle=stream.handle)\n",
    "        # Transfer predictions back from GPU\n",
    "        cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
    "        # Synchronize the stream\n",
    "        stream.synchronize()\n",
    "\n",
    "#         # synchronous execution\n",
    "#         # copy inputs to cuda memory\n",
    "#         cuda.memcpy_htod(d_inputs[0], input_ids.astype(np.int32))\n",
    "#         cuda.memcpy_htod(d_inputs[1], segment_ids.astype(np.int32))\n",
    "#         cuda.memcpy_htod(d_inputs[2], input_mask.astype(np.int32))\n",
    "#         # run inference synchronously\n",
    "#         context.execute(bindings=[int(d_inp) for d_inp in d_inputs] + [int(d_output)],batch_size=batch_size)    \n",
    "#         # transfer back from gpu\n",
    "#         cuda.memcpy_dtoh(h_output, d_output)\n",
    "#         #Problems for synchronous execution: output zeros\n",
    "\n",
    "        eval_time_elapsed = time.time() - eval_start_time\n",
    "        if step == 0:\n",
    "            cold_start_time = time.time() - cold_start\n",
    "        ttl_time += eval_time_elapsed\n",
    "#         correct += (h_output.reshape(batch_size,22).argmax(axis=1) == label).astype(int).sum()\n",
    "    eval_time_elapsed = ttl_time / (step + 1)\n",
    "    print(\"-----------------------------\")\n",
    "    print(\"Running Inference in {:.3f} Batches/Sec\".format(\n",
    "        1.0/eval_time_elapsed\n",
    "    ))\n",
    "    print(\"Time using for one batch is {:3f} ms\".format(eval_time_elapsed*1000))\n",
    "    print(\"Average time using for one inference is {:3f} ms\".format(eval_time_elapsed*1000/batch_size))\n",
    "    print(\"Time using for one batch cold start is {:.3f} s\".format(cold_start_time))\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004367828369140625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer_time - eval_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatively, bert_inference script can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python bert_inference.py -e bert_shopee_8_batch32.engine -pf /workspace/TensorRT/demo/BERT/eval_ckpts/data.txt -v /workspace/models/fine-tuned/bert_tf_v2_base_fp16_128_v2/vocab.txt -b 32 -s 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
